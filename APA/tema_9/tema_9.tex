\chapter{Mètodes de conjunt (``ensemble")}
\section{Introducció}

\begin{itemize}
	\item Un conjunt de models pot arribar a generalitzar millor que un dels models del conjunt triat a l'atzar.
	\item Quan un model és dolent:
	\begin{itemize}
		\item té massa biaix (infrajustat)
		\item té massa variància (sobreajustat)
	\end{itemize}
\end{itemize}

Hi ha dos grans grups de mètodes per resoldre aquests problemes:
\begin{itemize}
	\item \textbf{Bagging}: Per disminuir la variància
	\item \textbf{Boosting}: Per reduir el biaix
\end{itemize}

\section{Bagging (Bootstrap Aggregating)}
Si es té tot un conjunt de models i es volen integrar en un de sol, els models individuals han de presentar poc biaix i molta variància. 

\begin{itemize}
	\item Arbres de decisió
	\item Xarxes neuronals (cost computacional gran)
\end{itemize}

\begin{itemize}
	\item Es vol generar diversitat entre els models individuals
	\item Idealment, es necessitaria una mostra de dades \textbf{diferent} per cada model individual.
	\item No es disposa d'aquesta quantitat de mostres per tant es re-mostreja la única mostra que es té.
\end{itemize}


Imaginem que es té un conjunt de dades $\mathcal{D}$

\begin{align*}
	&\mathcal{D} = \{ (x_1, t_1),..., (x_N, t_N) \} \\
	& \hat{\theta} = \theta(\mathcal{D}) \text{ del veritable (poblacional) }, \theta \\
	& Z_1^* \{ 1, ..., N \} \ \text{(amb re-emplaç)}\ \hat{\theta}_1^* = \theta(Z_1^*) \\
	& Z_2^* \{ 1, ..., N \} \ \text{(amb re-emplaç)}\ \hat{\theta}_2^* = \theta(Z_2^*) \\
	& \vdots \\
	& Z_B^* \{ 1, ..., N \} \ \text{(amb re-emplaç)}\ \hat{\theta}_B^* = \theta(Z_B^*) \\
\end{align*}
$$
\hat{\theta}^* = \frac{1}{B} \sum_{i=1}^B \hat{\theta}_i^*
$$


\subsection{Bagging (D,B)}
\begin{algorithmic}
	\State $\varepsilon := \phi$
	\For{$i := 1$ \textbf{to} $B$}
		\State generar $Z_i^*$ de $\mathcal{D}$
		\State ajustar un model a $Z_i^* \to M_i$
		\State $\varepsilon := \varepsilon \cup \{ M_i \}$
	\EndFor
\end{algorithmic}

% FIGURES 1 i 2

\section{Random Forest}
Un \texttt{Random Forest} és Baggings on els models individuals són arbres de decisió i a més, s'afegeix aleatorietat en el procés de construcció dels arbres individuals.

Si es tenen unes dades que tenen $d$ variables, per cada node de cada arbre es seleccionen $m << d$ variables a l'atzar i es selecciona la millor per expansió.

\begin{itemize}
	\item $m = \sqrt{d} \quad$ (Classificació)
	\item $m = \frac{d}{3} \quad$ (regressió)
\end{itemize}

\subsection{Predicció (RF, x)}
\begin{algorithmic}
	\ForAll{element $e$ \textbf{in} $\mathcal{D}$}
		\State Escollir tots els model que no utilitzen $e$
		\State Calcular l'error dels models seleccionats
	\EndFor
\end{algorithmic}
