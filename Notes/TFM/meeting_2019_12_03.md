# Meeting 2019-12-03

## Intuition about higher priority formulation

$$
\begin{aligned}
t_{high}(J_i) &= \min_{J_j \in \{hp_i\} \cap \{\mathcal{J} \setminus \mathcal{J}^v\}}\{t_{cores}(J_i, J_j)\} \\
t_{cores}(J_i, J_j) &= \begin{cases}
r_j^{\max} & \text{ if } s_j^{\min} \le s_i^{\max} \\
\max\{r_j^{\max}, A_{s_j^{\min}}^{\max}\} & \text{ otherwise}
\end{cases}
\end{aligned}
$$

Proof

- First part:
  - Assumptions: $s_j^{\min} \le s_i^{\min}$, $r_j^{\max} \le r_i$ and $J_j \in hp_i \cap \{\mathcal{J} \setminus \mathcal{J}^v\}$. 
  - By contradiction, assume that $J_i$ starts executing before $J_j$
  - Let $t$ be the time at which $J_i$ starts executing. At least $s_i^{\min}$ cores are available at $t$ to execute $J_i$. At least $s_j^{\min} \le s_i^{\min}$ cores are available to execute $J_j$. 
  - Since $r_j^{\max} \le r_i$, by the (Job-level fixed-priority) JLFP scheduling policy, $J_j$ is chosen first by the scheduler to be dispatched at time $t$
  - This contradicts the assumption the $J_i$ starts executing before $J_j$. Either $r_j^{\max} > r_i$ or $s_j^{\min} > s_i^{\min}$
- Second part (v1):
  - Assumptions: $s_j^{\min} > s_i^{\min}$, $r_j^{\max} \le r_i$, $A_{s_j^{\min}}^{\max} \le r_i$ and $J_j \in hp_i \cap \{\mathcal{J} \setminus \mathcal{J}^v\}$
  - By contradiction, assume that $J_i$ starts executing before $J_j$
  - Let $t$ be the time at which $J_i$ starts executing. At least $s_j^{\min}$ cores are available, since $A_{s_j^{\min}}^{\max} \le r_i$, to execute $J_j$.
  - Since $r_j^{\max} \le r_i$, by the JLFP scheduling policy, $J_j$ is chosen first by the scheduler to be dispatched at time $t$
  - This contradicts the assumption that $J_i$ starts executing before $J_j$. Either $r_j^{\max} > r_i$, $A_{s_j^{\min}}^{\max} > r_i$ or $s_j^{\min} \le s_i^{\min}$
- Second part (v2):
  - Assumptions: $s_j^{\min} > s_i^{\min}$ and $A_{s_i^{\min}}^{\max} < r_i^{\max} < A_{s_j^{\min}}^{\max}$ and $J_j \in hp_i \cap \{\mathcal{J} \setminus \mathcal{J}^v\}$
  - By contradiction, assume that $J_j$ starts executing before $J_i$
  - Let $t$ be the time at which $J_j$ starts executing. At least $s_i^{\min} < s_j^{\min}$ cores are available to execute $J_j$ and $t \ge \max\{r_j, A_{s_j^{\min}}^{\max}\}$
  - Since $t \ge A_{s_j^{\min}}^{\max} > r_i^{\max} > A_{s_i^{\min}}^{\max}$, by the JLFPFor the sake of simplicity we consider periodic tasks and we assume that Ci is the exact
    duration of the task τi.For the sake of simplicity we consider periodic tasks and we assume that Ci is the exact
    duration of the task τi. scheduling policy $J_i$ should have been chosen first by the scheduler to be dispatched at time $\max\{r_i^{\max}, A_{s_i^{\min}}^{\max} \}$ instead of time $t$
  - This contradicts the assumption that $J_j$ starts executing before $J_i$.

## Papers

### An Optimal Multiprocessor Scheduling Algorithm without Fairness (2006)

Geoffrey Nelissen, Vandy Berten, Joël Goosens

#### Conclusions

- This is not Gang scheduling
- **Not related**

### Gang EDF scheduling of Parallel Task Systems (2009)

Shinpei Kato, Yutaka Ishikawa

- Coscheduling
- Number of cores:
  - **Rigid**: number of processors fixed a priory
  - **Moldable**: Number of processors not fixed but determined before execution
  - **Malleable**: Number of processors can change at runtime

#### Goals

- Create Gang EDF
- Provide schedulability analysis of Gang EDF
- Provide guarantees for all tasks to be schedulable when $C_i$ and $v_i$ are given

#### Assumptions

- Task generates same number of threads as used processors before the execution
- Each thread is handled as an individual task
- Task: $t_i = (v_i, C_i, D_i, T_i)$
  - No jitter used, only Worst Case Execution Time $C_i$
  - No release jitter
  - Constrained-deadline case $D_i \le T_i \rightarrow$ what's this?
  - Total execution time: $C_i \times v_i$ rectangle
  - $C_i$ is determined by $v_i$
- Jobs are **preemptive with no cost**

#### How do they do it

![Coscheduling](images/2019/12/03/02_coscheduling.png){width=75%}



![Gang scheduling](images/2019/12/03/02_gang.png){width=75%}



- Scheduler:
  - Same as Global EDF: *jobs with earlier deadlines are assigned higher priorities*
    - Take into account spacial limitation on the number of available processors
  - Two scheduling policies for a task $\tau_i$ with less than $v_i$ available cores
    - Coscheduling: schedule first only the available threads and schedule the remaining ones when more cores are available. *Appropriate for malleable task model*. It can also increase the total execution time if some threads require synchronization
    - Gang: block all the threads of $\tau_i$ until more or equal $v_i$ cores are available
  - Gang EDF:
    - Selects the job with the earliest deadline
    - If the job cannot start due to spatial limitations select next job **according to first fit heuristic**
- Analysis:
  - [BAR] test
  - Case by case demonstration
  - They provide schedulability conditions


#### Conclusions

- **Related**

### A Loadable Real-Time Scheduler Suite for Multicore Platforms (2009)

Shinpei Kato, Ragunathan (Raj) Rajkumar, and Yutaka Ishikawa

#### Goals

- Linux support for Real-Time schedulers
  - Runtime schedulability
  - Fixed-priority preemptive scheduling policies
  - Multicore Platforms
- Not modify Linux kernel source code

#### Assumptions

- Works on the Linux kernel
- Tasks **do not have parallelism**
- $C_i \le D_i \le T_i$
- There are no shared resources and critical sections

#### How do they do it

- They create an API so the user can interact with the RT scheduler that is a module on top of the linux kernel
- Implemented schedulers:
  - Partitioned scheduling: Each CPU has its own scheduler
  - Semi-partitioned scheduling: Tasks are assigned to CPUs except some tasks that are allowed migration
  - Global scheduling
- Develop SchedBench a schedulability benchmark tool

#### Conclusions

- It works
- Gang scheduling (EDF) not implemented yet
- **Related but not useful**

### Gang FTP scheduling of periodic and parallel rigid real-time tasks (2010)

Joël Goossens, Vandy Berten

#### Goals

- Implement Fixed Task Priority Gang scheduler. Sub-classes:
  - Parallelism Monotonic
  - Idling
  - Limited Gang
  - Limited Slack Reclaiming
- Study predictability of the schedulers
- Provide schedulability test (exact)

#### Assumptions

- Identical multiprocessor platforms
- Parallel real-time tasks
- Preemptions enabled
- Predictable schedules
- Jobs can have execution jitter

#### How do they do it

- Parallelism Monotonic
  - Set priorities depending on the job's number of cores
- Idling Scheduler
  - If the task finishes earlier than the Worst Case then leave the processor idling
- Limited Gang FJP Scheduler
  - Schedule job only if the previous one has been scheduled before
- Gang FJP and Limited Slack Reclaiming
  - Slack server of size $l$. For tasks smaller than $l$ if there are at least $l$ schedule this tasks in the *Slack server*.

#### Conclusions

- **Why is predictability important?**
- **Related**

### Optimal Scheduling of Periodic Gang Tasks (2016)

Joël Goossens, Pascal Richard

#### Goals

- Create DP-Fair scheduler
- Numerical experiments

#### Assumptions

- $J_i = (r_i, s_i, c_i, d_i)$
- $\tau_i = (s_i, C_i, T_i)$
- Preemptions enabled
- Periodic tasks
- Jobs are gang
- No jitter
- They care about fairness

#### How do they do it

- DP-Wrap:
  - Optimal for tasks with implicit deadlines
  - Considers time continuous
  - Distribution of tasks is related to the utilization
- DP-Fair:
  - Extension of DP-Wrap but with the whole hyper-period
- gang-h: Fixed-task priority scheduling rule
  - Priorities are assigned in non-increasing order of the number of requested processors
  - Scheduling decisions are taken every time a job is released or completed. At such event, all tasks are preempted and the current priority list is used to allocate ready tasks to the processors greedily as feasibly while the current job can be scheduled on the remaining available processors.
- Numerical experiments: Using Linear Programming
  - They compute the optimal schedule and then compare it against the gang-h

#### Conclusions

- **Related but not useful**

### Analysis Techniques for supporting hard real-time sporadic gang task systems (2019)

Zheng Dong, Cong Liu

#### Goals

- Create a utilization-based schedulability test for hard real-time gang task systems
- Partitioning scheme that enables a set of gang tasks to be efficiently assigned and scheduled

#### Assumptions

- Sporadic gang tasks
- 
- Preemptive Global EDF

#### How do they do it

- For the schedulability test they use a case by case demonstration
- Partitioning system for multiple clusters

#### Conclusions

- **Related but not useful**
- The gang task partitioning is based on multiple clusters of $M$ processors

### Adaptive Rate Control through Elastic Scheduling (2000)

Girogio Buttazzo, Luca Abeni

#### Goals

- Load balancing through an elastic task model
- Automatically adapting the rates without forcing the programmer to provide a priori estimates of tasks' computation times

#### Assumptions

- Tasks utilizations are treated as springs with given elastic coefficients
- Tasks can vary period 

#### How do they do it

- Kernel computes WCET and mean execution time of a task
- The period of the task is adapted depending on the elasticity $E_i$ of the task and the WCET $C_i$
- The utilization is the actual value that acts like a *spring*
- Specific algorithm to compute periods of all the tasks together
- Tasks period is computed atomically as a high priority task

#### Conclusions

- It is a good starter for elastic scheduling
- **Related**

### Adaptive Workload Management through Elastic Scheduling (2002)

Giorgio Buttazzo, Luca Abeni

#### Goals

- Compare task $\tau_i$ with a linear spring $S_i$ characterized by an elastic coefficient $E_i$, a nominal length $x_{i_0}$ and a minimum length $x_{i_{\min}}$. $x_i$ is the task utilization factor $U_i = C_i / T_i$.
- There is the set of fixed springs $\Gamma_f$ and the set of variable springs $\Gamma_v$
- Direct mapping between the spring length $x_i$ and the task utilization $U_i$

#### Assumptions

- Same as previous paper

#### How do they do it

- Same as previous paper with some variants

#### Conclusions

- Very similar to the paper from Buttazzo (2000) but explained more in depth
- **Related**

### Generalized elastic scheduling for real-time tasks

Thidapat Chantem, Xiaobo Sharon Hu, Michael D. Lemmon

#### Goals

- Generalize the existing elastic scheduling approach in several directions
- Extend to cases where task deadlines are less than task periods
- Find a set of task deadlines such that the task set becomes feasible

#### Assumptions

- Tasks can change period
- Tasks can change deadlines
- Global EDF scheduler

#### How do they do it

- Introduce $L_i$ which is the length of a job period
- Treat the spring problem as an optimization problem that can be solved faster
- Use heuristic to speed up computations. The idea is that we do not want to get stuck so any heuristic is better than not heuristic as that can always give an advantage.

#### Conclusions

- **Related**: it could be useful as they define the problem formally

### Elastic Scheduling for Parallel Real-Time Systems

James Orr, Chris Gill, Kunal Agrawal, Jing Li, Sanjoy Baruah

#### Goals

- Extend the elastic scheduling algorithm to multiple processors

#### Assumptions

- Platform with multiple processors
- Federated scheduling paradigm. Each task whose computational demand exceeds the capacity of a single processor is granted exclusive access to multiple processors.
  - These tasks are granted exclusive access to a subset of processors
- Tasks can adapt their periods in response to system behavior
- Task $\tau_i = (C_i, T_i^{(\min)}, T_i^{(\max)}, E_i)$
- Higher elastic coefficient $E_i$ implies a more elastic task
- We have the span of the execution (longest path) $L_i$

#### How do they do it

- Define $\lambda$ compression factor permitted

- After every scheduling period compute new $U_i$ for every task and the number of cores in relation to formula

- $$
  m_i = \left\lceil \frac{C_i - L_i}{T_i - L_i} \right\rceil
  $$

#### Conclusions

- It can be what we were already proposing to do
- **Related 100%**

## Next meeting

- Explain all the difficulties regarding the schedulability
- Sustainable test: A schedulability test is sustainable if any task in the system deemed schedulable by the test, remains so if it behaves "better" than mandated by its system specifications.
- Send list of papers