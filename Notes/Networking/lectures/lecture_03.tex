% !TEX root = ../main.tex

\section{Spectrum}

\subsection{Eigenvalues and eigenvectors}

$$
Ax = \lambda x
$$

\begin{itemize}
  \item An eigenvector $x$ is a non-zero vector belonging to eigenvalue $\lambda$
  \item An $n \times n$ matrix $A$ has $n$ eigenvalues (possibly not all distinct)
  \item The eigenvalue problem for the transposed matrix leads us to talk about 
  right-eigenvectors ($A$ and $A^T$ have the same eigenvalues, but not necessarily the same
  eigenvectors) 
  \item The eigenvectors are linearly independent
  \item Mostly, we normalized eigenvectors such that $x^Tx = 1$
  \item If $P^{-1}$ exists, then $PAP^{-1}$ has the same eigenvalues as $A$, but each 
  eigenvector is $y = Px$, where $x$ is an eigenvector of $A$
\end{itemize}

\begin{align*}
  A(rx) = \lambda (rx) \quad r \ne 1
\end{align*}

\begin{gather*}
  A 
  \begin{bmatrix}
    x_1 & x_2 & \cdots & x_N
  \end{bmatrix} =
  \begin{bmatrix}
    x_1 & x_2 & \cdots & x_N
  \end{bmatrix}
  \begin{bmatrix}
    \lambda_1 & & & \\
    & \lambda_2 & & \\
    & & \ddots & \\
    & & & \lambda_N
  \end{bmatrix} \\
  AX = X\Lambda \implies A = X\Lambda X^{-1} \\
  (A - \lambda I)x = 0 \implies \det(A - \lambda I) = 0
\end{gather*}

\subsubsection{Basic theorem for symmetric matrices}
\textbf{Any real symmetric matrix $S$} can be written as $S = X \Lambda X^T$,
where $X$ is the orthogonal matrix with real eigenvectors in the columns and 
$\Lambda = diag(\lambda_1, ..., \lambda_N)$, where $\lambda_j$ is the $j$-th real eigenvalue.

The real eigenvalues can be ordered as

\begin{gather*}
  \lambda_N \le \lambda_{N-1} \le \cdots \lambda_2 \le \lambda_1
\end{gather*}

If symmetric: $A = A^T = X \Lambda X^T = \sum_{k=1}^{N} \lambda_k x_k x^T_k$

\subsubsection{The orthogonal matrix X of a symmetric matrix A}

$$
A^T = \sum_{k=1}^N \lambda_k^m x_k x^T_k
$$

Orthogonality of eigenvectors: $x_k^T x_m = \delta_{km} \implies X^T X = I \implies X^{-1} = X^T$

A matrix and its inverse commute: $X^{-1}X = X X^{-1} \implies X^TX = XX^T = I$ \textbf{Double
orthogonality}. Both column vectors (=eigenvectors of A) and row vectors of $X$ are orthogonal.

\subsubsection{Gerschgorin's theorem}

It explains why we use spectral radious..

\begin{quotation}
  Each eigenvalue of an $n\times n$ matrix $A$ lies in at least one of the circular discs
  with a center $a_{jj}$ and radii $R_j = \sum_{k=1;k \ne j}^n |a_{jk}|$
\end{quotation}

\textbf{Consequence for Adjacency matrix $A$: } $|\lambda| \le \sum_{k=1}^n a_{rk} = d_r$
$$
\lambda_1 \le d_{max}
$$

The largest eigenvalue is always smaller thant the maximum degree

\textbf{Properties}:
\begin{itemize}
  \item Spectrum of $A$:
  \begin{itemize}
    \item All eigenvalues lie in the interval $(-d_{max}, d_{max}]$
    \item $\sum_{j=1}^N \lambda_j = 0 \qquad \sum_{j=1}^N \lambda_j^2 = 2L \qquad
    \sum_{j=1}^N \lambda_j^k = Trace(A^k)$
    \item Perron-Frobenius Theorem: $\lambda_1$ non-negative and components eigenvector $x_1$ are
    non-negative. (irreducible = connected: positive)
  \end{itemize}
  \item Spectrum of $Q = \Delta - A = BB^T$:
  \begin{itemize}
    \item Any eigenvalue $\mu_k$ is non-negative and the smallest $\mu_N = 0$
    \item Complexity (number of spanning trees is) $\epsilon(G) = \frac{1}{N} \prod_{k=1}^{N-1} \mu_k$
    \item The second smallest eigenvalue of the Laplacian $Q$, called \textbf{the algebraic 
    connectivity} $a(G) = \mu_{N-1}$, quantifies how strongly a graph is 
  \end{itemize}
\end{itemize}

\subsection{Largest eigenvalue of a symmetric matrix} 

If $Ax = \lambda x$ then $A^k x = \lambda^k x$ for non-negative integers $k$.

\textbf{Power method} (eigenvector): 
$$
A^k w = \alpha_1 \lambda_1^k x_1 \left(1 + \left(\left|\frac{\lambda_2}{\lambda_1}
\right|^k\right)\right)
$$

\textbf{Gerschgorin's theorem}: $\lambda_1 \le d_{max}$

\textbf{Rayleigh's principle}: $\lambda_1 \ge \frac{w^T Aw}{w^Tw}$ with equality only if $w = x_1$

\subsubsection{Spectral radius adjacency matrix}
Rayleigh principle: $\lambda_1 \ge \frac{w^T Aw}{w^TW}$ with equality only if $w = x_1$

Choose $w = u$, then $u^Tu = N$ and $u^TAu = 2L$

Hence:
$$
d_{max} \ge \lambda_1 (A) \ge \frac{2L}{N} = E[D]
$$

Equality holds if $u$ is an eigenvector of $A$. In general, $A u = d$ if $d = r u$, then 
$u$ is an eigenvector. 

The eigenvalue equation $A x = \lambda x$ implies that $A^m x = \lambda^m x$

Rayleigh principle

Choose $w = u$ and $m = 2$ then $u^Tu = N$ and 
$u^T A^2 u= (Au)^T (Au) = d^Td = \sum_{j = 1}^N d^2_j$

$$
\lambda_1 \ge E[D]\sqrt{1 + \frac{Var[D]}{(E[D])^2}} \ge E[D]
$$

Tighter bound thant the previous one

\subsubsection{Spectral radius and subgraph structure}
Let's have a graph $G$ that has two subgraphs $G_1$ and $G_2$

$$
A = 
\begin{bmatrix}
  A_1 & C \\
  C^T & A_2
\end{bmatrix}
\implies \lambda_1(G) \ge \lambda_1(G_1)
$$

\subsection{Graph metric: Eigenvector centrality}

\begin{itemize}
  \item The $i$-th component $(x_j)_i$ of the eigenvector $x_j$ belonging to the $j$-th 
  largest eigenvalue $\lambda_j$ of the adjacency matrix $A$ reflects a property of node 
  $i$ in the graph at ``eigenfrequency'' $\lambda_j$
  \begin{itemize}
    \item Hence $(x_j)_i^2$ is non-zero and can be interpreted as a centrality metrics for node $i$
  \end{itemize}
\end{itemize}



